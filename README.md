
## Causal Modeling with Stationary Diffusions (AISTATS 2024)

**This branch is intended for reproducing the results in [Lorch et al. (2024)](https://arxiv.org/abs/2310.17405). 
The code in this branch not maintained or updated anymore and may contain outdated 
notation and documentation.**


## Installation
To install the dependencies, create a new conda environment with
```
conda env create --file environment.yml
```
and then run
```
conda activate stadion
```
You then need to install the `stadion` package and also `cdt-source` (adapted from `cdt`) with
```
pip install -e .
pip install -e ./cdt-source/
```
To install the baselines requiring R, install R and then install all R packages via
``` 
Rscript r_install.R
```


## Results

All benchmarking results are generated through the script `manager.py`, which runs
the different scripts needed to run the experiments, by following the instructions below.
In total, we run six experiments, which are configured by separate folders in `config/`.
```
scm-er
scm-sf
linear-er
linear-sf
sergio-er
sergio-sf
```
The experiment folders contain `.yaml` files that specify the data-generating process (`data.yaml`),
the hyperparameter grids searched over for each of the methods (`methods_validation.yaml`), 
as well as the final hyperparameter configuration each of the methods is evaluated with (`methods.yaml`).

Running each of the above experiments consists of running five commands sequentially.
```
python manager.py <experiment-name> --data --submit
python manager.py <experiment-name> --methods_train_validation --n_datasets 20 --submit
python manager.py <experiment-name> --summary_train_validation --n_datasets 20 --submit
python manager.py <experiment-name> --methods --submit
python manager.py <experiment-name> --summary --submit
```
After the `--summary_train_validation` step, the summary folder lists the best hyperparameter 
configuration for each method on the train/validation data split.
Before proceeding, we copy the optimal hyperparameters manually into the 
`methods.yaml` config files of the respective experiment 
folder to run experiments on the test data.
Ultimately, the results of each of the five experiments calls are saved in their own 
subfolder of the `results/` directory.
The `--submit` flag launches a given experiment, otherwise we get a printout of what would be run.


## Figures 

The figures in the paper are generated by running the following scripts:
```
python figure_one.py
python witness_visualization.py
python plot_results.py
```
The summary files used in `plot_results.py` are generated by running the `manager.py` scripts above.
The directory names of the summary dataframes have to be updated with the final names in `plot_results.py`.

